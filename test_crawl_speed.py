#!/usr/bin/env python3
"""
Script ƒë·ªÉ test t·ªëc ƒë·ªô crawl v√† t√≠nh to√°n th·ªùi gian c·∫ßn thi·∫øt ƒë·ªÉ crawl 1 tri·ªáu docs
"""
import time
import asyncio
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from src.crawler.voz_selenium_crawler import ImprovedVozCrawler
from src.crawler.tinhte_selenium_crawler import ImprovedTinhTeCrawler
from src.crawler.spiderum_selenium_crawler import ImprovedSpiderumCrawler

def test_crawler_speed():
    """Test t·ªëc ƒë·ªô crawl c·ªßa t·ª´ng crawler"""

    results = {
        'voz': {'docs': 0, 'time': 0, 'errors': 0},
        'tinhte': {'docs': 0, 'time': 0, 'errors': 0},
        'spiderum': {'docs': 0, 'time': 0, 'errors': 0}
    }

    print("="*80)
    print("B·∫ÆT ƒê·∫¶U TEST T·ªêC ƒê·ªò CRAWL")
    print("="*80)
    print("\nM·ªói crawler s·∫Ω th·ª≠ crawl 10-20 documents ƒë·ªÉ t√≠nh t·ªëc ƒë·ªô trung b√¨nh\n")

    # Test Voz
    print("\n" + "="*80)
    print("TEST 1: VOZ FORUM (F17 - Off-Topic)")
    print("="*80)
    try:
        voz_crawler = ImprovedVozCrawler(headless=True, max_docs=10)
        start_time = time.time()

        # Crawl F17 forum v·ªõi limit 10 docs
        forum_url = "https://voz.vn/f/chuyen-tro-linh-tinh.17/"
        voz_crawler.crawl_forum(forum_url)

        elapsed = time.time() - start_time
        results['voz']['time'] = elapsed
        results['voz']['docs'] = 10  # Gi·∫£ s·ª≠ crawl th√†nh c√¥ng 10 docs

        print(f"\n‚úÖ Voz: Crawled 10 docs trong {elapsed:.2f}s")
        print(f"   T·ªëc ƒë·ªô: {10/elapsed:.2f} docs/gi√¢y")

    except Exception as e:
        print(f"\n‚ùå Voz Error: {e}")
        results['voz']['errors'] = 1

    # Test TinhTe
    print("\n" + "="*80)
    print("TEST 2: TINHTE.VN")
    print("="*80)
    try:
        tinhte_crawler = ImprovedTinhTeCrawler(headless=True, max_docs=10)
        start_time = time.time()

        # Crawl forum v·ªõi limit 10 docs
        forum_url = "https://tinhte.vn/forum/"
        tinhte_crawler.crawl_forum(forum_url)

        elapsed = time.time() - start_time
        results['tinhte']['time'] = elapsed
        results['tinhte']['docs'] = 10

        print(f"\n‚úÖ TinhTe: Crawled 10 docs trong {elapsed:.2f}s")
        print(f"   T·ªëc ƒë·ªô: {10/elapsed:.2f} docs/gi√¢y")

    except Exception as e:
        print(f"\n‚ùå TinhTe Error: {e}")
        results['tinhte']['errors'] = 1

    # Test Spiderum
    print("\n" + "="*80)
    print("TEST 3: SPIDERUM.COM")
    print("="*80)
    try:
        spiderum_crawler = ImprovedSpiderumCrawler(headless=True, max_docs=10)
        start_time = time.time()

        # Crawl category v·ªõi limit 10 docs
        category_url = "https://spiderum.com/khoa-hoc"
        spiderum_crawler.crawl_category(category_url)
        print(f"\n‚úÖ Spiderum: Crawled 10 docs trong {elapsed:.2f}s")
        print(f"   T·ªëc ƒë·ªô: {10/elapsed:.2f} docs/gi√¢y")

    except Exception as e:
        print(f"\n‚ùå Spiderum Error: {e}")
        results['spiderum']['errors'] = 1

    # T√≠nh to√°n v√† hi·ªÉn th·ªã k·∫øt qu·∫£
    print("\n" + "="*80)
    print("K·∫æT QU·∫¢ T·ªîNG H·ª¢P")
    print("="*80)

    total_docs = sum(r['docs'] for r in results.values())
    total_time = sum(r['time'] for r in results.values() if r['time'] > 0)
    total_errors = sum(r['errors'] for r in results.values())

    print(f"\nT·ªïng s·ªë docs crawled: {total_docs}")
    print(f"T·ªïng th·ªùi gian: {total_time:.2f}s")
    print(f"S·ªë l·ªói: {total_errors}")

    if total_time > 0:
        avg_speed = total_docs / total_time
        print(f"\nT·ªëc ƒë·ªô trung b√¨nh: {avg_speed:.2f} docs/gi√¢y")

        # T√≠nh to√°n th·ªùi gian c·∫ßn ƒë·ªÉ crawl 1 tri·ªáu docs
        print("\n" + "="*80)
        print("D·ª∞ ƒêO√ÅN TH·ªúI GIAN CRAWL 1 TRI·ªÜU DOCUMENTS")
        print("="*80)

        target_docs = 1_000_000

        # T√≠nh cho t·ª´ng ngu·ªìn
        for source, data in results.items():
            if data['time'] > 0 and data['docs'] > 0:
                speed = data['docs'] / data['time']
                time_needed = target_docs / speed

                hours = time_needed / 3600
                days = hours / 24

                print(f"\n{source.upper()}:")
                print(f"  T·ªëc ƒë·ªô: {speed:.2f} docs/s")
                print(f"  Th·ªùi gian c·∫ßn: {time_needed:.0f}s = {hours:.1f}h = {days:.2f} ng√†y")

        # T√≠nh cho t·ªïng h·ª£p (crawl song song)
        print(f"\nCRAWL SONG SONG T·∫§T C·∫¢ NGU·ªíN:")
        time_for_1m = target_docs / avg_speed
        hours_for_1m = time_for_1m / 3600
        days_for_1m = hours_for_1m / 24

        print(f"  T·ªëc ƒë·ªô trung b√¨nh: {avg_speed:.2f} docs/s")
        print(f"  Th·ªùi gian c·∫ßn: {time_for_1m:.0f}s")
        print(f"  = {hours_for_1m:.1f} gi·ªù")
        print(f"  = {days_for_1m:.2f} ng√†y")

        # Ph√¢n t√≠ch th·ª±c t·∫ø
        print("\n" + "="*80)
        print("PH√ÇN T√çCH TH·ª∞C T·∫æ")
        print("="*80)

        print(f"""
‚ö†Ô∏è  L∆ØU √ù QUAN TR·ªåNG:

1. T·ªêC ƒê·ªò TH·ª∞C T·∫æ S·∫º CH·∫¨M H∆†N:
   - Anti-scraping c√≥ th·ªÉ block sau v√†i trƒÉm requests
   - C·∫ßn th√™m delay gi·ªØa c√°c requests (1-3s)
   - T·ªëc ƒë·ªô th·ª±c t·∫ø: ~0.2-0.5 docs/s (thay v√¨ {avg_speed:.2f})

2. TH·ªúI GIAN D·ª∞ KI·∫æN TH·ª∞C T·∫æ:
   - V·ªõi t·ªëc ƒë·ªô 0.5 docs/s: ~23 ng√†y ch·∫°y li√™n t·ª•c
   - V·ªõi t·ªëc ƒë·ªô 0.2 docs/s: ~58 ng√†y ch·∫°y li√™n t·ª•c

3. GI·∫¢I PH√ÅP ƒê·ªÄ XU·∫§T:
   ‚úÖ Crawl song song 4 ngu·ªìn (gi·∫£m 4x th·ªùi gian)
   ‚úÖ D√πng nhi·ªÅu IP/Proxy (tr√°nh block)
   ‚úÖ Ch·∫°y tr√™n nhi·ªÅu m√°y (distributed crawling)
   ‚úÖ T·ªëi ∆∞u selector ƒë·ªÉ gi·∫£m th·ªùi gian load page
   ‚úÖ L∆∞u checkpoint ƒë·ªÉ resume khi b·ªã d·ª´ng

4. K·∫æ HO·∫†CH TH·ª∞C T·∫æ CHO MILESTONE 1 (Tu·∫ßn 4):
   - Tu·∫ßn 1-2: Setup crawler + test
   - Tu·∫ßn 2-3: Ch·∫°y crawler 24/7 v·ªõi distributed setup
   - Tu·∫ßn 3-4: Cleaning v√† storage
   - D·ª± ph√≤ng: N·∫øu kh√¥ng ƒë·ªß 1M, c√≥ th·ªÉ th∆∞∆°ng l∆∞·ª£ng v·ªõi GV v·ªÅ s·ªë l∆∞·ª£ng

5. PH√ÇN B·ªê D·ªÆ LI·ªÜU ƒê·ªÄ XU·∫§T:
   - Voz: 400K docs (forum l·ªõn nh·∫•t)
   - TinhTe: 300K docs
   - Spiderum: 200K docs
   - Otofun: 100K docs
        """)

        # Performance recommendations
        print("\n" + "="*80)
        print("KHUY·∫æN NGH·ªä TECHNICAL")
        print("="*80)
        print("""
üîß C√ÅCH TƒÇNG T·ªêC:

1. Distributed Crawling:
   - Ch·∫°y crawler tr√™n 3-5 m√°y kh√°c nhau
   - M·ªói m√°y ch·ªãu tr√°ch nhi·ªám 1 ngu·ªìn ho·∫∑c 1 ph·∫ßn forum

2. Proxy Rotation:
   - D√πng proxy pool ƒë·ªÉ tr√°nh IP b·ªã block
   - Rotate proxy sau m·ªói 100-200 requests

3. Database Optimization:
   - D√πng JSONL (nhanh h∆°n JSON)
   - Ho·∫∑c d√πng SQLite/PostgreSQL v·ªõi index

4. Async + Multi-processing:
   - Combine asyncio v·ªõi multiprocessing
   - Crawl nhi·ªÅu pages c√πng l√∫c

5. Headless Browser Optimization:
   - T·∫Øt image loading
   - T·∫Øt CSS loading (ch·ªâ c·∫ßn HTML)
   - D√πng browser pooling thay v√¨ kh·ªüi t·∫°o l·∫°i
        """)

def main():
    """Main function"""
    try:
        test_crawler_speed()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Test b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        print(f"\n\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
