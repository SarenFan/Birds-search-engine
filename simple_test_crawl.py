#!/usr/bin/env python3
"""
Simple crawl speed test - thá»­ má»™t vÃ i threads vÃ  tÃ­nh toÃ¡n thá»i gian
"""
import time
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent / 'src'))

from src.crawler.selenium_utils import SeleniumCrawler
from src.crawler.voz_selenium_crawler import ImprovedVozCrawler

def main():
    print("="*80)
    print("CRAWL SPEED TEST - VOZ FORUM")
    print("="*80)
    print("\nÄang khá»Ÿi Ä‘á»™ng crawler...")

    try:
        # Setup crawler
        crawler = ImprovedVozCrawler(
            output_file='data_sample/speed_test.jsonl',
            max_docs=10,
            headless=True
        )

        selenium_driver = SeleniumCrawler(headless=True)

        # Start timing
        start_time = time.time()

        # Crawl forum
        print("\nBáº¯t Ä‘áº§u crawl Voz F17 (Off-Topic)...")
        print("Target: 10 documents\n")

        crawler.crawl_forum(
            crawler=selenium_driver,
            forum_name="Off-Topic",
            forum_url="https://voz.vn/f/chuyen-tro-linh-tinh.17/",
            max_pages=2
        )

        # End timing
        elapsed = time.time() - start_time
        docs_collected = len(crawler.collected_docs)

        # Save data
        crawler.save_data()

        # Close browser
        selenium_driver.close()

        # Results
        print("\n" + "="*80)
        print("Káº¾T QUáº¢ TEST")
        print("="*80)
        print(f"\nâœ… ÄÃ£ crawl: {docs_collected} documents")
        print(f"â±ï¸  Thá»i gian: {elapsed:.2f} giÃ¢y")

        if elapsed > 0 and docs_collected > 0:
            speed = docs_collected / elapsed
            print(f"ğŸš€ Tá»‘c Ä‘á»™: {speed:.3f} docs/giÃ¢y")

            # Calculate time for 1 million
            print("\n" + "="*80)
            print("Dá»° ÄOÃN THá»œI GIAN CHO 1 TRIá»†U DOCS")
            print("="*80)

            target = 1_000_000
            seconds_needed = target / speed
            hours = seconds_needed / 3600
            days = hours / 24

            print(f"\nğŸ“Š Vá»›i tá»‘c Ä‘á»™ {speed:.3f} docs/s:")
            print(f"   â€¢ Thá»i gian: {seconds_needed:,.0f} giÃ¢y")
            print(f"   â€¢ Tá»©c lÃ : {hours:,.1f} giá»")
            print(f"   â€¢ Hoáº·c: {days:,.2f} ngÃ y (cháº¡y liÃªn tá»¥c 24/7)")

            # Realistic analysis
            print("\n" + "="*80)
            print("PHÃ‚N TÃCH THá»°C Táº¾")
            print("="*80)

            realistic_speed = 0.3  # docs/s (slower due to delays, errors, etc)
            realistic_seconds = target / realistic_speed
            realistic_hours = realistic_seconds / 3600
            realistic_days = realistic_hours / 24

            print(f"\nâš ï¸  Tá»‘c Ä‘á»™ thá»±c táº¿ (vá»›i delays & anti-scraping): ~{realistic_speed} docs/s")
            print(f"   â€¢ Thá»i gian: {realistic_seconds:,.0f} giÃ¢y")
            print(f"   â€¢ Tá»©c lÃ : {realistic_hours:,.1f} giá»")
            print(f"   â€¢ Hoáº·c: {realistic_days:,.2f} ngÃ y")

            # Multi-source analysis
            print("\nğŸ“Œ CRAWL SONG SONG 4 NGUá»’N:")
            parallel_days = realistic_days / 4
            print(f"   â€¢ Voz, TinhTe, Spiderum, Otofun cÃ¹ng lÃºc")
            print(f"   â€¢ Thá»i gian giáº£m xuá»‘ng: ~{parallel_days:.1f} ngÃ y")

            print("\nğŸ’¡ KHUYáº¾N NGHá»Š:")
            print("   1. Crawl song song 4 nguá»“n â†’ Giáº£m 4x thá»i gian")
            print("   2. Sá»­ dá»¥ng proxy rotation â†’ TrÃ¡nh bá»‹ block")
            print("   3. Cháº¡y trÃªn nhiá»u mÃ¡y â†’ TÄƒng tá»‘c Ä‘á»™")
            print("   4. Báº¯t Ä‘áº§u sá»›m (tuáº§n 1-2) â†’ CÃ³ thá»i gian dá»± phÃ²ng")

            # Timeline
            print("\n" + "="*80)
            print("Káº¾ HOáº CH Äá»€ XUáº¤T CHO MILESTONE 1 (TUáº¦N 4)")
            print("="*80)
            print("""
ğŸ“… TUáº¦N 1 (Hiá»‡n táº¡i):
   â€¢ Setup vÃ  test crawlers
   â€¢ Äiá»u chá»‰nh selectors
   â€¢ Test vá»›i 100-1000 docs

ğŸ“… TUáº¦N 2-3:
   â€¢ Báº¯t Ä‘áº§u crawl chÃ­nh thá»©c
   â€¢ Cháº¡y 24/7 trÃªn 4 nguá»“n
   â€¢ Monitor vÃ  fix lá»—i
   â€¢ Backup dá»¯ liá»‡u thÆ°á»ng xuyÃªn

ğŸ“… TUáº¦N 4:
   â€¢ HoÃ n thiá»‡n data cleaning
   â€¢ TÃ¡ch tá»« vÃ  de-duplication
   â€¢ Táº¡o bÃ¡o cÃ¡o thá»‘ng kÃª
   â€¢ Chuáº©n bá»‹ demo

ğŸ¯ Má»¤C TIÃŠU:
   â€¢ Voz: 400K docs
   â€¢ TinhTe: 300K docs
   â€¢ Spiderum: 200K docs
   â€¢ Otofun: 100K docs
   â€¢ Tá»”NG: 1.000.000 docs
            """)

        print("\n" + "="*80)
        print(f"Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {crawler.output_file}")
        print("="*80)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  Test bá»‹ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
