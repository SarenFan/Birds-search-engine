# CHIáº¾N LÆ¯á»¢C CRAWL 1 TRIá»†U DOCS Vá»šI 1 MÃY TÃNH

## ğŸ¯ Má»¤C TIÃŠU

- Crawl 1,000,000 documents tá»« 4 nguá»“n
- Cháº¡y ngáº¯t quÃ£ng (cÃ³ thá»ƒ dá»«ng/tiáº¿p tá»¥c)
- Ban Ä‘Ãªm: Cháº¡y crawler
- Ban ngÃ y: Dá»«ng crawler, dÃ¹ng mÃ¡y cho viá»‡c khÃ¡c

---

## ğŸ“Š PHÃ‚N TÃCH VÃ€ Káº¾ HOáº CH

### Thá»i Gian Kháº£ Dá»¥ng

- **Ban Ä‘Ãªm:** 10 giá»/ngÃ y (22:00 - 08:00)
- **Cuá»‘i tuáº§n:** 20 giá»/ngÃ y (cÃ³ thá»ƒ cháº¡y cáº£ ngÃ y)
- **Tá»•ng:** ~90-100 giá»/tuáº§n

### Tá»‘c Äá»™ Cáº§n Thiáº¿t

```
Target: 1,000,000 docs trong 3 tuáº§n (Tuáº§n 2-4)
Thá»i gian kháº£ dá»¥ng: ~270 giá» (3 tuáº§n Ã— 90h/tuáº§n)

Tá»‘c Ä‘á»™ cáº§n: 1,000,000 / (270 Ã— 3600) = 1.03 docs/giÃ¢y
Thá»±c táº¿ vá»›i overhead: Cáº§n ~1.5-2 docs/giÃ¢y
```

### PhÃ¢n Bá»• Nguá»“n (4 crawlers song song)

| Nguá»“n    | Target  | Docs/giá» | Giá» cáº§n | Tuáº§n cáº§n  |
| -------- | ------- | -------- | ------- | --------- |
| Voz      | 400,000 | 1,500    | 267h    | 2.96 tuáº§n |
| TinhTe   | 300,000 | 1,100    | 273h    | 3.03 tuáº§n |
| Spiderum | 200,000 | 750      | 267h    | 2.96 tuáº§n |
| Otofun   | 100,000 | 375      | 267h    | 2.96 tuáº§n |

**Káº¿t luáº­n:** Náº¿u cháº¡y 4 sources SONG SONG, cÃ³ thá»ƒ hoÃ n thÃ nh trong 3 tuáº§n!

---

## ğŸš€ CHIáº¾N LÆ¯á»¢C Tá»I Æ¯U CHO 1 MÃY

### 1. MULTI-PROCESS ARCHITECTURE

```python
# Run 4 crawlers Ä‘á»“ng thá»i, má»—i crawler 1 process riÃªng
Process 1: Voz crawler
Process 2: TinhTe crawler
Process 3: Spiderum crawler
Process 4: Otofun crawler

Má»—i process:
- Ram usage: ~500MB
- CPU: 1 core
- Total: 2GB RAM, 4 cores (feasible trÃªn mÃ¡y thÆ°á»ng)
```

### 2. CHECKPOINT & RESUME SYSTEM

**Táº¡i sao quan trá»ng:**

- Dá»«ng crawler lÃºc 8h sÃ¡ng â†’ Resume lÃºc 10h tá»‘i
- MÃ¡y crash/máº¥t Ä‘iá»‡n â†’ KhÃ´ng máº¥t dá»¯ liá»‡u
- Track progress real-time

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

```json
// voz_checkpoint.json
{
  "last_forum": "F17",
  "last_page": 145,
  "last_thread": "https://voz.vn/t/...",
  "docs_collected": 45678,
  "seen_hashes": [...],
  "timestamp": "2026-01-11T08:00:00"
}
```

### 3. RESOURCE OPTIMIZATION

**A. Memory Management:**

```python
# Write data incrementally (má»—i 100 docs)
# Clear cache sau má»—i page
# DÃ¹ng generator thay vÃ¬ load all vÃ o memory
```

**B. Browser Optimization:**

```python
options.add_argument('--disable-images')  # Giáº£m 60% bandwidth
options.add_argument('--disable-css')     # Giáº£m 20% load time
options.add_argument('--disk-cache-size=0')  # KhÃ´ng cache
```

**C. Batch Processing:**

```python
# Thay vÃ¬ crawl tá»«ng thread:
# 1. Láº¥y list 100 thread URLs
# 2. Crawl parallel 4-5 threads cÃ¹ng lÃºc
# 3. Write batch 100 docs
```

---

## ğŸ› ï¸ IMPLEMENTATION

### File Structure

```
SEG301-Project/
â”œâ”€â”€ crawler_manager.py          # Main orchestrator
â”œâ”€â”€ night_crawler.py            # Auto start/stop theo schedule
â”œâ”€â”€ src/
â”‚   â””â”€â”€ crawler/
â”‚       â”œâ”€â”€ voz_crawler_v2.py       # Optimized version
â”‚       â”œâ”€â”€ tinhte_crawler_v2.py
â”‚       â”œâ”€â”€ spiderum_crawler_v2.py
â”‚       â””â”€â”€ otofun_crawler_v2.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ voz_data.jsonl          # Incremental write
â”‚   â”œâ”€â”€ tinhte_data.jsonl
â”‚   â”œâ”€â”€ spiderum_data.jsonl
â”‚   â””â”€â”€ otofun_data.jsonl
â””â”€â”€ checkpoints/
    â”œâ”€â”€ voz_checkpoint.json
    â”œâ”€â”€ tinhte_checkpoint.json
    â”œâ”€â”€ spiderum_checkpoint.json
    â””â”€â”€ otofun_checkpoint.json
```

### Main Orchestrator Script

**File: `crawler_manager.py`**

```python
#!/usr/bin/env python3
"""
Crawler Manager - Cháº¡y 4 crawlers Ä‘á»“ng thá»i vá»›i checkpoint/resume
"""
import multiprocessing as mp
from datetime import datetime
import time
import signal
import sys

class CrawlerManager:
    def __init__(self):
        self.processes = []
        self.should_stop = False

    def start_crawler(self, crawler_class, name, target_docs):
        """Start a crawler in separate process"""
        def run():
            crawler = crawler_class(
                output_file=f'data/{name}_data.jsonl',
                checkpoint_file=f'checkpoints/{name}_checkpoint.json',
                max_docs=target_docs
            )
            crawler.run()

        p = mp.Process(target=run, name=name)
        p.start()
        self.processes.append(p)
        print(f"âœ“ Started {name} crawler (PID: {p.pid})")

    def start_all(self):
        """Start all 4 crawlers"""
        print("="*80)
        print("STARTING ALL CRAWLERS")
        print("="*80)

        # Start each crawler
        self.start_crawler(VozCrawlerV2, 'voz', 400000)
        self.start_crawler(TinhTeCrawlerV2, 'tinhte', 300000)
        self.start_crawler(SpiderumCrawlerV2, 'spiderum', 200000)
        self.start_crawler(OtofunCrawlerV2, 'otofun', 100000)

        print(f"\nâœ“ All crawlers started at {datetime.now()}")

    def stop_all(self):
        """Gracefully stop all crawlers"""
        print("\n" + "="*80)
        print("STOPPING ALL CRAWLERS")
        print("="*80)

        for p in self.processes:
            if p.is_alive():
                print(f"Stopping {p.name}...")
                p.terminate()
                p.join(timeout=10)

        print("âœ“ All crawlers stopped")

    def monitor(self):
        """Monitor crawler progress"""
        try:
            while any(p.is_alive() for p in self.processes):
                time.sleep(60)  # Check every minute

                # Print status
                alive = [p.name for p in self.processes if p.is_alive()]
                print(f"[{datetime.now().strftime('%H:%M:%S')}] Running: {', '.join(alive)}")

        except KeyboardInterrupt:
            print("\nâš ï¸  Interrupted by user")
            self.stop_all()

if __name__ == "__main__":
    manager = CrawlerManager()

    # Handle Ctrl+C gracefully
    def signal_handler(sig, frame):
        manager.stop_all()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)

    # Start and monitor
    manager.start_all()
    manager.monitor()
```

### Auto Night Crawler Script

**File: `night_crawler.py`**

```python
#!/usr/bin/env python3
"""
Auto Night Crawler - Tá»± Ä‘á»™ng cháº¡y tá»« 22:00 Ä‘áº¿n 08:00
"""
from datetime import datetime, time
import subprocess
import time as t
import os

class NightCrawler:
    def __init__(self):
        self.start_time = time(22, 0)  # 10 PM
        self.end_time = time(8, 0)     # 8 AM
        self.process = None

    def is_night_time(self):
        """Check if current time is night time"""
        now = datetime.now().time()

        if self.start_time < self.end_time:
            # Normal case (e.g., 10:00 - 20:00)
            return self.start_time <= now <= self.end_time
        else:
            # Night case (e.g., 22:00 - 08:00)
            return now >= self.start_time or now <= self.end_time

    def start_crawlers(self):
        """Start crawler manager"""
        if self.process is None or self.process.poll() is not None:
            print(f"ğŸŒ™ Starting crawlers at {datetime.now()}")

            # Activate venv and run crawler manager
            cmd = "source venv/bin/activate && python3 crawler_manager.py"
            self.process = subprocess.Popen(
                cmd,
                shell=True,
                executable='/bin/bash',
                cwd=os.getcwd()
            )
            print(f"âœ“ Crawlers started (PID: {self.process.pid})")

    def stop_crawlers(self):
        """Stop crawler manager"""
        if self.process and self.process.poll() is None:
            print(f"ğŸŒ… Stopping crawlers at {datetime.now()}")
            self.process.terminate()
            self.process.wait(timeout=30)
            print("âœ“ Crawlers stopped")

    def run(self):
        """Main loop"""
        print("="*80)
        print("NIGHT CRAWLER SCHEDULER")
        print("="*80)
        print(f"Schedule: {self.start_time} - {self.end_time}")
        print("Press Ctrl+C to stop\n")

        try:
            while True:
                if self.is_night_time():
                    # Night time - should be running
                    self.start_crawlers()
                else:
                    # Day time - should be stopped
                    self.stop_crawlers()

                # Check every 5 minutes
                t.sleep(300)

        except KeyboardInterrupt:
            print("\nâš ï¸  Scheduler stopped by user")
            self.stop_crawlers()

if __name__ == "__main__":
    scheduler = NightCrawler()
    scheduler.run()
```

---

## ğŸ“‹ HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG

### Option 1: Manual Control (Khuyáº¿n nghá»‹ Ä‘á»ƒ há»c)

```bash
# 1. Start táº¥t cáº£ crawlers (cháº¡y khi cáº§n)
source venv/bin/activate
python3 crawler_manager.py

# 2. Stop (Ctrl+C hoáº·c)
pkill -f crawler_manager.py

# 3. Check progress
python3 -c "
import json
for source in ['voz', 'tinhte', 'spiderum', 'otofun']:
    with open(f'checkpoints/{source}_checkpoint.json') as f:
        data = json.load(f)
        print(f'{source}: {data[\"docs_collected\"]:,} docs')
"
```

### Option 2: Auto Night Crawler (Set and Forget)

```bash
# 1. Start scheduler (chá»‰ cháº¡y 1 láº§n)
source venv/bin/activate
nohup python3 night_crawler.py > night_crawler.log 2>&1 &

# Scheduler sáº½ tá»± Ä‘á»™ng:
# - Start crawlers lÃºc 22:00
# - Stop crawlers lÃºc 08:00
# - Láº·p láº¡i má»—i ngÃ y

# 2. Check log
tail -f night_crawler.log

# 3. Stop scheduler (khi cáº§n)
pkill -f night_crawler.py
```

### Option 3: Systemd Service (Advanced - Cháº¡y nhÆ° service)

```bash
# 1. Create service file
sudo nano /etc/systemd/system/seg301-crawler.service

# Content:
[Unit]
Description=SEG301 Social Listening Crawler
After=network.target

[Service]
Type=simple
User=kource
WorkingDirectory=/home/kource/Documents/SEG301
ExecStart=/home/kource/Documents/SEG301/venv/bin/python3 night_crawler.py
Restart=on-failure

[Install]
WantedBy=multi-user.target

# 2. Enable and start
sudo systemctl enable seg301-crawler
sudo systemctl start seg301-crawler

# 3. Check status
sudo systemctl status seg301-crawler

# 4. View logs
sudo journalctl -u seg301-crawler -f
```

---

## ğŸ“Š MONITORING & TRACKING

### Real-time Progress Dashboard

**File: `monitor_progress.py`**

```python
#!/usr/bin/env python3
"""
Real-time progress monitor
"""
import json
import time
from datetime import datetime
import os

def clear_screen():
    os.system('clear' if os.name == 'posix' else 'cls')

def load_checkpoint(source):
    try:
        with open(f'checkpoints/{source}_checkpoint.json') as f:
            return json.load(f)
    except:
        return {'docs_collected': 0, 'timestamp': 'N/A'}

def get_file_size(filename):
    try:
        return os.path.getsize(filename) / (1024*1024)  # MB
    except:
        return 0

while True:
    clear_screen()
    print("="*80)
    print(f"CRAWL PROGRESS MONITOR - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

    sources = [
        ('Voz', 400000),
        ('TinhTe', 300000),
        ('Spiderum', 200000),
        ('Otofun', 100000)
    ]

    total_collected = 0

    for name, target in sources:
        checkpoint = load_checkpoint(name.lower())
        collected = checkpoint.get('docs_collected', 0)
        total_collected += collected

        progress = (collected / target) * 100
        file_size = get_file_size(f'data/{name.lower()}_data.jsonl')

        bar_length = 40
        filled = int(bar_length * progress / 100)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

        print(f"\n{name:12} [{bar}] {progress:5.1f}%")
        print(f"  Collected: {collected:,} / {target:,} docs")
        print(f"  File size: {file_size:.1f} MB")
        print(f"  Last update: {checkpoint.get('timestamp', 'N/A')}")

    print("\n" + "="*80)
    print(f"TOTAL: {total_collected:,} / 1,000,000 docs ({total_collected/10000:.1f}%)")

    # Estimate completion
    if total_collected > 0:
        # Assume constant rate
        hours_passed = 10  # Adjust based on actual runtime
        rate = total_collected / hours_passed
        remaining = 1000000 - total_collected
        hours_left = remaining / rate if rate > 0 else 0
        days_left = hours_left / 10  # 10 hours per day

        print(f"Rate: {rate:.0f} docs/hour")
        print(f"ETA: {days_left:.1f} days ({hours_left/24:.1f} days 24/7)")

    print("="*80)
    print("Press Ctrl+C to exit")

    time.sleep(10)  # Update every 10 seconds
```

**Run monitor:**

```bash
source venv/bin/activate
python3 monitor_progress.py
```

---

## âš¡ OPTIMIZATION TIPS

### 1. TÄƒng Tá»‘c Äá»™ Crawl

**A. Parallel Thread Crawling:**

```python
# Trong má»—i crawler, thay vÃ¬ crawl tuáº§n tá»±:
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=3) as executor:
    futures = [executor.submit(crawl_thread, url) for url in thread_urls[:10]]
    results = [f.result() for f in futures]
```

**B. Reuse Browser:**

```python
# Thay vÃ¬ má»Ÿ/Ä‘Ã³ng browser má»—i page:
class PersistentBrowser:
    def __init__(self):
        self.driver = setup_driver()
        self.page_count = 0

    def get_page(self, url):
        self.driver.get(url)
        self.page_count += 1

        # Restart browser má»—i 100 pages Ä‘á»ƒ trÃ¡nh memory leak
        if self.page_count % 100 == 0:
            self.driver.quit()
            self.driver = setup_driver()
```

### 2. Giáº£m Resource Usage

**A. Headless Mode:**

```python
options.add_argument('--headless=new')  # No GUI
```

**B. Disable Unnecessary Features:**

```python
prefs = {
    'profile.default_content_settings': {'images': 2},  # No images
    'profile.managed_default_content_settings': {'images': 2}
}
options.add_experimental_option('prefs', prefs)
```

### 3. Smart Scheduling

**Daily Schedule:**

```
22:00 - 23:00  Crawl warm-up (check for issues)
23:00 - 07:00  Full speed crawling
07:00 - 08:00  Crawl cool-down (finish current batches)
08:00 - 22:00  Machine free for other use
```

**Weekend Boost:**

```
Cuá»‘i tuáº§n: Cháº¡y 20h/ngÃ y thay vÃ¬ 10h
â†’ CÃ³ thá»ƒ crawl gáº¥p Ä‘Ã´i
â†’ Giáº£m deadline stress
```

---

## ğŸ¯ Káº¾ HOáº CH 3 TUáº¦N

### Tuáº§n 1 (10-16 Jan): Setup & Testing

```
âœ… Day 1-2: Setup scripts, test crawlers
âœ… Day 3-4: Fix bugs, optimize
âœ… Day 5-7: Start crawling (target: 150K docs)
```

### Tuáº§n 2 (17-23 Jan): Main Crawling

```
â¬œ Cháº¡y full 10h/ngÃ y
â¬œ Target: 450K docs (total 600K)
â¬œ Monitor vÃ  fix issues
â¬œ Backup data má»—i ngÃ y
```

### Tuáº§n 3 (24-30 Jan): Final Push

```
â¬œ Cháº¡y full + thÃªm giá» náº¿u cáº§n
â¬œ Target: 400K docs (total 1M)
â¬œ Day 5-7: Data cleaning, deduplication
â¬œ Prepare demo vÃ  report
```

### Tuáº§n 4 (31 Jan - 6 Feb): Submission

```
â¬œ Finalize cleaned data
â¬œ Generate statistics
â¬œ Create presentation
â¬œ Submit Milestone 1
```

---

## ğŸ”§ TROUBLESHOOTING

### Issue 1: Crawler bá»‹ stop giá»¯a chá»«ng

```bash
# Check logs
tail -f night_crawler.log

# Restart manually
python3 crawler_manager.py
```

### Issue 2: Memory quÃ¡ cao

```bash
# Check memory usage
ps aux | grep python | awk '{print $6/1024 " MB - " $11}'

# Giáº£m sá»‘ browser instances
# Edit crawler_manager.py: Giáº£m tá»« 4 xuá»‘ng 2-3
```

### Issue 3: Disk Ä‘áº§y

```bash
# Check disk space
df -h

# Compress old data
gzip data/voz_data.jsonl

# Move to external drive
```

### Issue 4: IP bá»‹ block

```bash
# ThÃªm delay dÃ i hÆ¡n trong crawler
# human_like_delay(5, 10)  # Thay vÃ¬ 2-4

# Hoáº·c dÃ¹ng VPN
# sudo openvpn --config vpn-config.ovpn
```

---

## ğŸ“ˆ SUCCESS METRICS

### Daily Targets

```
Day 1-7:   ~50K docs/tuáº§n   (Setup phase)
Day 8-14:  ~400K docs/tuáº§n  (Main phase)
Day 15-21: ~450K docs/tuáº§n  (Final push)
Day 22-28: ~100K + cleanup  (Buffer)
```

### Quality Metrics

```
âœ“ Word count: >50 words per doc
âœ“ Uniqueness: <5% duplicates
âœ“ Valid Vietnamese: >95%
âœ“ Complete metadata: 100%
```

---

## ğŸ’¡ PRO TIPS

1. **Backup má»—i ngÃ y:**

   ```bash
   # Cron job backup
   0 9 * * * rsync -av /home/kource/Documents/SEG301/data/ /backup/seg301/
   ```

2. **Alert khi crawler stop:**

   ```python
   # Gá»­i email/Telegram notification
   if not any(p.is_alive() for p in processes):
       send_alert("All crawlers stopped!")
   ```

3. **Log rotation:**

   ```bash
   # TrÃ¡nh log file quÃ¡ lá»›n
   mv crawler.log crawler.log.old
   ```

4. **Test trÆ°á»›c khi sleep:**

   - Cháº¡y test 30 phÃºt trÆ°á»›c khi Ä‘i ngá»§
   - Äáº£m báº£o khÃ´ng cÃ³ lá»—i
   - Check progress dashboard

5. **Weekend advantage:**
   - Cuá»‘i tuáº§n cháº¡y full throttle
   - CÃ³ thá»ƒ boost tá»« 10h â†’ 20h/ngÃ y
   - Compensate náº¿u tuáº§n trÆ°á»›c thiáº¿u target

---

## ğŸ“ SUPPORT

Náº¿u gáº·p issue:

1. Check logs: `tail -f night_crawler.log`
2. Check progress: `python3 monitor_progress.py`
3. Check process: `ps aux | grep crawler`
4. Document trong AI log
5. Adjust strategy if needed

**Remember:** Flexibility is key! Adjust schedule based on actual progress.

---

**Last updated:** 2026-01-10
**Author:** Phan Minh Tai
**Status:** Ready for Implementation
