# H∆Ø·ªöNG D·∫™N SETUP CRAWLER TR√äN LIGHTNING.AI

## ‚úÖ X√ÅC NH·∫¨N: Lightning.ai C√ì BACKGROUND EXECUTION!

**Tin t·ªët:**

- ‚úÖ Lightning.ai H·ªñ TR·ª¢ background execution kh√¥ng gi·ªõi h·∫°n (unlimited)
- ‚úÖ B·∫°n c√≥ **22 credits** s·∫µn ($22 USD)
- ‚úÖ CPU Studios c√≥ th·ªÉ ch·∫°y mi·ªÖn ph√≠ (1 Studio free 24/7)
- ‚úÖ ƒê√≥ng browser v·∫´n ch·∫°y ng·∫ßm

---

## üí∞ T√çNH TO√ÅN CHI PH√ç V·ªöI 22 CREDITS

### CPU Pricing (T·ªëi ∆∞u cho Crawling)

**FREE Tier:**

- ‚úÖ **1 CPU Studio MI·ªÑN PH√ç 24/7** (kh√¥ng t·ªën credits!)
- ‚ö†Ô∏è Free Studio c·∫ßn restart m·ªói 4 gi·ªù
- ‚úÖ Unlimited background execution

**CPU Studios (Paid):**

- 4 CPU cores: ~$0.05/gi·ªù
- 8 CPU cores: ~$0.10/gi·ªù
- 16 CPU cores: ~$0.20/gi·ªù
- 32 CPU cores: ~$0.40/gi·ªù

### T√≠nh to√°n v·ªõi 22 Credits:

**Option 1: D√πng Free CPU Studio (KHUY·∫æN NGH·ªä)**

```
Cost: $0 (ho√†n to√†n mi·ªÖn ph√≠!)
Caveats:
- Restart m·ªói 4 gi·ªù (c√≥ checkpoint ‚Üí kh√¥ng v·∫•n ƒë·ªÅ)
- 4 CPU cores (ƒë·ªß cho 1-2 crawlers)

Timeline v·ªõi 4 cores:
- 1 crawler: ~7-10 ng√†y
- 2 crawlers parallel: ~10-14 ng√†y
```

**Option 2: D√πng 8-CPU Studio**

```
Cost: ~$0.10/gi·ªù
Timeline: 168 gi·ªù (7 ng√†y) = $16.80
‚Üí C√≤n l·∫°i: 22 - 16.80 = $5.20 credits

V·ªõi 8 cores:
- 2 crawlers parallel: ~5-7 ng√†y
- 4 crawlers parallel: ~7-10 ng√†y (c√≥ th·ªÉ b·ªã bottleneck network)
```

**Option 3: D√πng 16-CPU Studio (FASTEST)**

```
Cost: ~$0.20/gi·ªù
Timeline: 168 gi·ªù (7 ng√†y) = $33.60
‚ö†Ô∏è V∆Ø·ª¢
T QU√Å 22 credits!

Alternative: Ch·∫°y 100 gi·ªù (~4 ng√†y) = $20
‚Üí C√≤n $2 d·ª± ph√≤ng

V·ªõi 16 cores:
- 4 crawlers parallel: ~3-4 ng√†y
```

---

## üéØ KHUY·∫æN NGH·ªä: HYBRID STRATEGY

### BEST PLAN v·ªõi 22 Credits:

**Phase 1: Free CPU Studio (3-4 ng√†y) - $0**

```
Setup:
- 1 Free CPU Studio (4 cores)
- 2 crawlers parallel (Voz + TinhTe)
- Background execution enabled
- Auto-resume sau m·ªói 4h restart

Result: ~300-400K docs
Cost: $0
```

**Phase 2: Paid 8-CPU Studio (3-4 ng√†y) - $7-10**

```
Setup:
- 8 CPU Studio
- 4 crawlers parallel (Voz, TinhTe, Spiderum, Otofun)
- Full speed

Result: ~600-700K docs
Cost: ~$7-10
```

**TOTAL:**

- Time: 6-8 ng√†y
- Docs: 900K-1.1M docs ‚úì
- Cost: $7-10 (c√≤n $12-15 credits d·ª± ph√≤ng)

---

## üöÄ STEP-BY-STEP SETUP

### Step 1: T·∫°o Lightning.ai Account

1. V√†o [lightning.ai](https://lightning.ai)
2. Sign up (verify phone number)
3. Check balance: B·∫°n c√≥ 22 credits (15 free monthly + 7 purchased?)

### Step 2: Create New Studio

```bash
# V√†o https://studio.lightning.ai/
# Click "New Studio"
# Ch·ªçn: CPU Studio (4 cores - FREE)
# Name: seg301-crawler
```

### Step 3: Setup Environment

**Trong Studio terminal:**

```bash
# 1. Install system dependencies
sudo apt-get update
sudo apt-get install -y chromium-browser chromium-chromedriver

# 2. Clone repository
cd /teamspace/studios/this_studio
git clone https://github.com/SarenFan/Birds-search-engine.git
cd Birds-search-engine

# 3. Install Python dependencies
pip install -r requirements.txt
pip install selenium undetected-chromedriver beautifulsoup4 jsonlines fake-useragent

# 4. Verify ChromeDriver
which chromedriver
# Should output: /usr/bin/chromedriver
```

### Step 4: Create Lightning-Optimized Crawler Manager

**File: `lightning_crawler.py`**

```python
#!/usr/bin/env python3
"""
Crawler Manager for Lightning.ai
Optimized for background execution with checkpoints
"""
import multiprocessing as mp
import sys
import time
import signal
import json
from datetime import datetime
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from src.crawler.voz_selenium_crawler import ImprovedVozCrawler
from src.crawler.tinhte_selenium_crawler import ImprovedTinhTeCrawler
from src.crawler.spiderum_selenium_crawler import ImprovedSpiderumCrawler
from src.crawler.selenium_utils import SeleniumCrawler

class LightningCrawlerManager:
    """
    Crawler Manager t·ªëi ∆∞u cho Lightning.ai
    - Auto checkpoint m·ªói 30 ph√∫t
    - Resume after 4-hour restart
    - Resource monitoring
    """

    def __init__(self, data_dir="/teamspace/studios/this_studio/data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        self.checkpoint_dir = Path("/teamspace/studios/this_studio/checkpoints")
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        self.processes = []
        self.should_stop = False

    def run_voz_crawler(self):
        """Run Voz crawler"""
        try:
            print(f"[{datetime.now()}] Starting Voz crawler...")

            crawler = ImprovedVozCrawler(
                output_file=str(self.data_dir / 'voz_data.jsonl'),
                checkpoint_file=str(self.checkpoint_dir / 'voz_checkpoint.json'),
                max_docs=400000,
                headless=True
            )

            driver = SeleniumCrawler(headless=True)

            crawler.crawl_forum(
                crawler=driver,
                forum_name="F17-OffTopic",
                forum_url="https://voz.vn/f/chuyen-tro-linh-tinh.17/",
                max_pages=2000
            )

            driver.close()
            print(f"[{datetime.now()}] Voz crawler completed!")

        except Exception as e:
            print(f"[{datetime.now()}] Voz crawler error: {e}")
            import traceback
            traceback.print_exc()

    def run_tinhte_crawler(self):
        """Run TinhTe crawler"""
        try:
            print(f"[{datetime.now()}] Starting TinhTe crawler...")

            crawler = ImprovedTinhTeCrawler(
                output_file=str(self.data_dir / 'tinhte_data.jsonl'),
                checkpoint_file=str(self.checkpoint_dir / 'tinhte_checkpoint.json'),
                max_docs=300000,
                headless=True
            )

            driver = SeleniumCrawler(headless=True)

            crawler.crawl_forum(
                crawler=driver,
                forum_url="https://tinhte.vn/forums/",
                max_pages=1500
            )

            driver.close()
            print(f"[{datetime.now()}] TinhTe crawler completed!")

        except Exception as e:
            print(f"[{datetime.now()}] TinhTe crawler error: {e}")
            import traceback
            traceback.print_exc()

    def run_spiderum_crawler(self):
        """Run Spiderum crawler"""
        try:
            print(f"[{datetime.now()}] Starting Spiderum crawler...")

            crawler = ImprovedSpiderumCrawler(
                output_file=str(self.data_dir / 'spiderum_data.jsonl'),
                checkpoint_file=str(self.checkpoint_dir / 'spiderum_checkpoint.json'),
                max_docs=200000,
                headless=True
            )

            driver = SeleniumCrawler(headless=True)

            crawler.crawl_category(
                crawler=driver,
                category_url="https://spiderum.com/khoa-hoc"
            )

            driver.close()
            print(f"[{datetime.now()}] Spiderum crawler completed!")

        except Exception as e:
            print(f"[{datetime.now()}] Spiderum crawler error: {e}")
            import traceback
            traceback.print_exc()

    def start_crawlers(self, num_parallel=2):
        """
        Start crawlers in parallel

        Args:
            num_parallel: Number of crawlers to run in parallel
                         2 for FREE 4-core Studio
                         4 for PAID 8+ core Studio
        """
        print("="*80)
        print("LIGHTNING.AI CRAWLER MANAGER")
        print("="*80)
        print(f"Starting {num_parallel} crawlers in parallel...")
        print(f"Data directory: {self.data_dir}")
        print(f"Checkpoint directory: {self.checkpoint_dir}")
        print(f"Background execution: ENABLED")
        print("="*80)

        # Create process list based on num_parallel
        crawler_funcs = [
            self.run_voz_crawler,
            self.run_tinhte_crawler,
            self.run_spiderum_crawler,
        ][:num_parallel]

        # Start processes
        for func in crawler_funcs:
            p = mp.Process(target=func, name=func.__name__)
            p.start()
            self.processes.append(p)
            print(f"‚úì Started {func.__name__} (PID: {p.pid})")
            time.sleep(5)  # Stagger starts

        print(f"\n‚úì All {len(self.processes)} crawlers started!")
        print("You can now close the browser - crawlers will run in background")
        print("\nTo check progress:")
        print("  cat checkpoints/*_checkpoint.json")
        print("\nTo monitor:")
        print("  watch -n 60 'cat checkpoints/*_checkpoint.json'")

    def monitor(self):
        """Monitor crawler progress"""
        try:
            while any(p.is_alive() for p in self.processes):
                time.sleep(300)  # Check every 5 minutes

                # Print status
                alive = [p.name for p in self.processes if p.is_alive()]
                if alive:
                    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Running: {', '.join(alive)}")

                    # Print progress from checkpoints
                    for checkpoint_file in self.checkpoint_dir.glob('*_checkpoint.json'):
                        try:
                            with open(checkpoint_file) as f:
                                data = json.load(f)
                                print(f"  {checkpoint_file.stem}: {data.get('docs_collected', 0):,} docs")
                        except:
                            pass

            print("\n" + "="*80)
            print("ALL CRAWLERS COMPLETED!")
            print("="*80)

        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Interrupted by user")
            self.stop_crawlers()

    def stop_crawlers(self):
        """Gracefully stop all crawlers"""
        print("\nStopping crawlers...")
        for p in self.processes:
            if p.is_alive():
                p.terminate()
                p.join(timeout=30)
        print("‚úì All crawlers stopped")

def main():
    """Main entry point"""
    manager = LightningCrawlerManager()

    # Handle signals
    def signal_handler(sig, frame):
        manager.stop_crawlers()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # Determine number of parallel crawlers based on cores
    import os
    cpu_count = os.cpu_count() or 4

    if cpu_count <= 4:
        num_parallel = 2  # Free Studio
        print("‚ö° Running on FREE 4-core Studio ‚Üí 2 parallel crawlers")
    elif cpu_count <= 8:
        num_parallel = 3  # 8-core Studio
        print("‚ö° Running on 8-core Studio ‚Üí 3 parallel crawlers")
    else:
        num_parallel = 4  # 16+ core Studio
        print("‚ö° Running on 16+ core Studio ‚Üí 4 parallel crawlers")

    # Start and monitor
    manager.start_crawlers(num_parallel=num_parallel)
    manager.monitor()

if __name__ == "__main__":
    main()
```

### Step 5: Enable Background Execution & Run

**Trong Lightning Studio:**

1. **Enable Background Execution:**

   - Click "Settings" (gear icon)
   - Enable "Background execution"
   - Save
2. **Run Crawler:**

```bash
# Start in terminal
cd /teamspace/studios/this_studio/Birds-search-engine
python3 lightning_crawler.py
```

3. **Verify Running:**

```bash
# Check processes
ps aux | grep python | grep lightning_crawler

# Check progress (trong terminal m·ªõi)
watch -n 60 'cat /teamspace/studios/this_studio/checkpoints/*_checkpoint.json'
```

4. **Close Browser:**
   - ƒê√≥ng browser tab
   - Crawlers v·∫´n ch·∫°y ng·∫ßm!

---

## üìä MONITORING FROM ANYWHERE

### Check Progress via Terminal (SSH)

```bash
# SSH v√†o Studio (Lightning cung c·∫•p SSH access)
ssh <your-studio-id>

# Check checkpoint files
cat /teamspace/studios/this_studio/checkpoints/voz_checkpoint.json

# Output example:
# {
#   "docs_collected": 125430,
#   "last_page": 567,
#   "timestamp": "2026-01-11T15:23:45"
# }
```

### Check Progress via Studio UI

1. V√†o [lightning.ai/studios](https://lightning.ai/studios)
2. Click v√†o Studio "seg301-crawler"
3. Open terminal
4. Run: `cat checkpoints/*_checkpoint.json`

### Download Data Progress

```bash
# Check data size
du -sh /teamspace/studios/this_studio/data/*.jsonl

# Output:
# 2.3G    voz_data.jsonl
# 1.8G    tinhte_data.jsonl
# 1.1G    spiderum_data.jsonl
```

---

## üíæ DOWNLOAD DATA V·ªÄ M√ÅY

### Option 1: Via Lightning UI (D·ªÖ nh·∫•t)

```bash
# Trong Studio terminal, compress data
cd /teamspace/studios/this_studio/data
tar -czf seg301_data.tar.gz *.jsonl

# Click v√†o file seg301_data.tar.gz
# Right-click ‚Üí Download
```

### Option 2: Via SCP (Nhanh h∆°n)

```bash
# Tr√™n m√°y local
scp -r <lightning-studio-ssh>:/teamspace/studios/this_studio/data/*.jsonl ./local/data/
```

### Option 3: Via Lightning Drive

```bash
# Upload to Lightning Drive (shared storage)
cp /teamspace/studios/this_studio/data/*.jsonl /teamspace/drive/

# Download from any Studio or via UI
```

---

## ‚ö†Ô∏è HANDLE 4-HOUR RESTART (Free Tier)

**Lightning Free Studio restart m·ªói 4 gi·ªù. ƒê√¢y l√† c√°ch handle:**

### Auto-Resume Script

**File: `auto_resume.sh`**

```bash
#!/bin/bash
# Auto-resume crawler after Studio restart

echo "üîÑ Checking if crawler is running..."

if ! pgrep -f "lightning_crawler.py" > /dev/null; then
    echo "‚ö° Starting crawler..."
    cd /teamspace/studios/this_studio/Birds-search-engine
    nohup python3 lightning_crawler.py > crawler.log 2>&1 &
    echo "‚úì Crawler started! PID: $!"
else
    echo "‚úì Crawler already running"
fi
```

**Setup auto-resume:**

```bash
# Make executable
chmod +x /teamspace/studios/this_studio/auto_resume.sh

# Add to Studio startup (Lightning feature)
# Settings ‚Üí On-start actions ‚Üí Add script
/teamspace/studios/this_studio/auto_resume.sh
```

**Ho·∫∑c manual restart sau m·ªói 4h:**

```bash
# Khi Studio restart, ch·ªâ c·∫ßn run l·∫°i:
cd /teamspace/studios/this_studio/Birds-search-engine
python3 lightning_crawler.py

# Checkpoint system s·∫Ω t·ª± ƒë·ªông resume t·ª´ n∆°i d·ª´ng!
```

---

## üéØ OPTIMIZATION TIPS

### 1. T·ªëi ∆Øu Memory (RAM)

```python
# Trong crawler code, th√™m:
import gc

def crawl_with_memory_management():
    for i, page in enumerate(pages):
        # Crawl page
        crawl_page(page)

        # Clear memory m·ªói 100 pages
        if i % 100 == 0:
            gc.collect()
```

### 2. T·ªëi ∆Øu Network

```python
# Disable images v√† CSS trong Selenium
options.add_argument('--blink-settings=imagesEnabled=false')
prefs = {'profile.default_content_settings': {'images': 2}}
options.add_experimental_option('prefs', prefs)
```

### 3. Monitor Resource Usage

```bash
# Check CPU/RAM usage
htop

# Check disk usage
df -h

# Check network
iftop
```

---

## üìà EXPECTED TIMELINE V·ªöI LIGHTNING.AI

### Scenario 1: FREE 4-Core Studio Only

```
Timeline: 7-10 ng√†y
Strategy:
- Day 1-5: 2 crawlers (Voz + TinhTe) ‚Üí 500K docs
- Day 6-10: 2 crawlers (Spiderum + Otofun*) ‚Üí 300K docs
- Manual restart m·ªói 4h (ho·∫∑c d√πng auto-resume)

Cost: $0
Total docs: 800K-900K
```

\*Note: Otofun c√≥ th·ªÉ c√≥ v·∫•n ƒë·ªÅ, c√≥ th·ªÉ thay = crawl th√™m t·ª´ Voz/TinhTe

### Scenario 2: Hybrid (Free + Paid 8-Core)

```
Phase 1 (Free 4-core): 3-4 ng√†y
- 2 crawlers ‚Üí 350K docs
- Cost: $0

Phase 2 (Paid 8-core): 3-4 ng√†y
- 4 crawlers ‚Üí 650K docs
- Cost: ~$7-10

Total: 6-8 ng√†y, 1M docs, $7-10
C√≤n l·∫°i: $12-15 credits d·ª± ph√≤ng
```

### Scenario 3: All-In 16-Core (Fastest)

```
Timeline: 3-4 ng√†y
- 4 crawlers parallel full speed
- ~250K docs/ng√†y

Cost: ~$20 (all 22 credits)
Risk: N·∫øu c√≥ l·ªói, kh√¥ng c√≤n credit d·ª± ph√≤ng
```

---

## ‚úÖ KHUY·∫æN NGH·ªä CU·ªêI C√ôNG

**BEST STRATEGY cho b·∫°n v·ªõi 22 credits:**

1. **Week 1 (Day 1-4): FREE Studio**

   - Cost: $0
   - Setup + Test + Run 2 crawlers
   - Target: 300-400K docs
   - Learn the platform
2. **Week 2 (Day 5-8): Paid 8-Core**

   - Cost: $7-10
   - Scale up to 4 crawlers
   - Target: 600-700K docs
   - Total: 1M docs ‚úì
3. **Reserve $12-15 credits:**

   - D·ª± ph√≤ng cho errors
   - Ho·∫∑c final push n·∫øu thi·∫øu

**ROI Analysis:**

- $10 ƒë·ªÉ c√≥ 1M docs trong 8 ng√†y
- Kh√¥ng c·∫ßn lo m√°y c√° nh√¢n
- Background execution
- Professional cloud infrastructure

---

## üÜò TROUBLESHOOTING

### Issue 1: Studio b·ªã restart sau 4h

**Solution:**

```bash
# Setup auto-resume script (xem tr√™n)
# Ho·∫∑c manual restart - checkpoint s·∫Ω resume t·ª± ƒë·ªông
python3 lightning_crawler.py
```

### Issue 2: Out of disk space

**Solution:**

```bash
# Check usage
df -h

# Compress old data
gzip /teamspace/studios/this_studio/data/*.jsonl

# Or delete checkpoints c≈©
rm /teamspace/studios/this_studio/checkpoints/*.old
```

### Issue 3: Crawler b·ªã stuck

**Solution:**

```bash
# Kill process
pkill -9 -f lightning_crawler

# Restart
python3 lightning_crawler.py

# Checkpoint s·∫Ω resume t·ª´ n∆°i d·ª´ng
```

### Issue 4: H·∫øt credits

**Solution:**

- Continue v·ªõi Free Studio (ch·∫≠m h∆°n nh∆∞ng v·∫´n ch·∫°y ƒë∆∞·ª£c)
- Ho·∫∑c upgrade/buy th√™m credits

---

## üìû SUPPORT

**Lightning.ai Resources:**

- Documentation: https://lightning.ai/docs
- Discord: https://lightning.ai/discord
- Support: support@lightning.ai

**Project Repository:**

- GitHub: https://github.com/SarenFan/Birds-search-engine
- AI Log: PhanMinhTai_ai_log.md

---

**FINAL CHECKLIST:**

- ‚úÖ ƒê√£ t·∫°o Lightning.ai account
- ‚úÖ Verified 22 credits available
- ‚úÖ Understood background execution
- ‚úÖ Planned hybrid strategy (Free + Paid)
- ‚úÖ Ready to setup
- ‚úÖ Estimated timeline: 6-8 ng√†y
- ‚úÖ Budget: $10, c√≤n $12 d·ª± ph√≤ng

üéØ **READY TO START!** B·∫Øt ƒë·∫ßu setup ngay h√¥m nay!

---

**Created:** 2026-01-10
**Author:** Phan Minh Tai
**Status:** Ready for Implementation on Lightning.ai
